% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/corpus_download.r
\name{corpus_download}
\alias{corpus_download}
\title{Download Corpus}
\usage{
corpus_download(
  pages_dir = file.path(".", "data", "pages"),
  title_and_abstract_search,
  continue = TRUE,
  delete_pages_dir = FALSE,
  select_fields = c("id", "doi", "authorships", "publication_year", "display_name",
    "abstract_inverted_index", "topics"),
  verbose = TRUE,
  dry_run = FALSE,
  mc_cores = 3,
  ...
)
}
\arguments{
\item{pages_dir}{The directory where the downloaded pages will be stored. Default is "./data/pages".}

\item{title_and_abstract_search}{The search query for the title and abstract of the documents to be downloaded.}

\item{continue}{Logical indicating whether to continue downloading from where it left off. Default is TRUE.}

\item{delete_pages_dir}{Logical indicating whether to delete the pages directory before downloading.
Default is FALSE.}

\item{select_fields}{A character vector of fields to be selected from the downloaded data. The fields
included in the default are
`id`, `doi`, `authorships`, `publication_year`, `display_name`, `abstract_inverted_index`, and `topics`.
These are needed to use the function `corpuus_pages_to_arrow()`.}

\item{verbose}{Logical indicating whether to display progress messages. Default is TRUE.}

\item{dry_run}{Logical indicating whether to run the function without downloading any data. Default is FALSE.}

\item{mc_cores}{The number of cores to be used for parallel processing. Default is 3.
This is limiting the number of parallel downloads}

\item{...}{Additional filter arguments.}
}
\value{
None
}
\description{
This function downloads a corpus of documents based on a given title and abstract search query.
}
\examples{
\dontrun{
download_corpus(title_and_abstract_search = "climate change")
}
}
